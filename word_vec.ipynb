{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rvheb\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset is sourced from [Kaggle](kaggle.com). <br>\n",
    "It is the [fake-and-real-news-dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv) </br>\n",
    "\n",
    "Since the dataset is split into two parts Fake.csv and True.csv , we combine the datasets into one and add a new column to indicate whether it is true or false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WATCH: Female Cop Halts Sex Offender’s Violen...</td>\n",
       "      <td>Earlier in January, 31-year-old Michael Cox wa...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 1, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senate confirms two FERC commissioners, restor...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. Senate on Thur...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>August 4, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AWESOME! Conservative Artist Crashes Anti-Trum...</td>\n",
       "      <td>Our favorite conservative street artist Sabo c...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 13, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clarence Thomas On The Bench Without Scalia I...</td>\n",
       "      <td>Supreme Court Justice Clarence Thomas is the e...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 22, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUH? NYT EDITOR Blames “Republican Rage Machin...</td>\n",
       "      <td>Talk about projecting! On Fareed Zakaria s CNN...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Jun 18, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   WATCH: Female Cop Halts Sex Offender’s Violen...   \n",
       "1  Senate confirms two FERC commissioners, restor...   \n",
       "2  AWESOME! Conservative Artist Crashes Anti-Trum...   \n",
       "3   Clarence Thomas On The Bench Without Scalia I...   \n",
       "4  HUH? NYT EDITOR Blames “Republican Rage Machin...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Earlier in January, 31-year-old Michael Cox wa...          News   \n",
       "1  WASHINGTON (Reuters) - The U.S. Senate on Thur...  politicsNews   \n",
       "2  Our favorite conservative street artist Sabo c...      politics   \n",
       "3  Supreme Court Justice Clarence Thomas is the e...          News   \n",
       "4  Talk about projecting! On Fareed Zakaria s CNN...      politics   \n",
       "\n",
       "                date  label  \n",
       "0   February 1, 2016      0  \n",
       "1    August 4, 2017       1  \n",
       "2       Nov 13, 2017      0  \n",
       "3  February 22, 2016      0  \n",
       "4       Jun 18, 2017      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df = pd.read_csv('Fake.csv')\n",
    "true_df = pd.read_csv('True.csv')\n",
    "\n",
    "fake_df['label'] = 0\n",
    "true_df['label'] = 1\n",
    "\n",
    "combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Earlier in January, 31-year-old Michael Cox wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. Senate on Thur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our favorite conservative street artist Sabo c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Supreme Court Justice Clarence Thomas is the e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Talk about projecting! On Fareed Zakaria s CNN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Earlier in January, 31-year-old Michael Cox wa...      0\n",
       "1  WASHINGTON (Reuters) - The U.S. Senate on Thur...      1\n",
       "2  Our favorite conservative street artist Sabo c...      0\n",
       "3  Supreme Court Justice Clarence Thomas is the e...      0\n",
       "4  Talk about projecting! On Fareed Zakaria s CNN...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = combined_df[['text', 'label']]\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split, val_split = train_test_split(combined_df, train_size=0.8, random_state=1)\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "train_split, test_split = train_test_split(combined_df, random_state=1, test_size=1 - train_ratio)\n",
    "val_split, test_split = train_test_split(test_split, test_size=test_ratio/(test_ratio + validation_ratio),random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = train_split['text']\n",
    "y_train = train_split['label']\n",
    "\n",
    "val_reviews = val_split['text']\n",
    "y_val = val_split['label']\n",
    "\n",
    "test_reviews = test_split['text']\n",
    "y_test = test_split['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 17675, 1: 15998})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "Preprocessing the data with some simple filters to remove numbers and special characters and convert all words to lower case. <br>\n",
    "The majority of the preprocessing section is handled by the Word Vectors which are coded later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=20000,\n",
    "                                               filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                                               lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 67, 37, 8, 35, 333, 77, 492, 198, 2, 2640, 1, 3393, 1720, 2634, 233, 26, 84, 2, 239, 2645, 1, 599, 9, 621, 285, 109, 1764, 4, 4215, 4272, 1, 91, 72, 12, 9, 207, 1643, 39, 22, 319, 7, 198, 21, 2, 8451, 16, 31, 76, 808, 18240, 7, 1086, 2, 226, 1, 35, 1294, 7, 11, 1, 817, 1487, 7, 34, 23, 1, 1281, 2249, 6, 1, 1417, 2, 2645, 1, 599, 3, 1, 1820, 10, 285, 134, 99, 5, 34, 113, 1098, 90, 7, 47, 91, 72, 398, 1634, 3559, 12, 582, 35, 1098, 1294, 26, 233, 113, 896, 26, 84, 47]\n"
     ]
    }
   ],
   "source": [
    "# converting the input to a sequence of integers instead of a bag of words\n",
    "# each word is represneted by a different integer\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_reviews)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['washington', 'reuters', 'u', 's', 'president'], [107, 67, 37, 8, 35])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.index_word[x] for x in X_train[0][:5]] , X_train[0][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Inputs\n",
    "\n",
    "After the input text has been converted into a sequence of numbers, we further normalize the text so that all articles have a uniform length, which, after some hit-and-trial was found to be around 250. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 250\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0   107\n",
      "    67    37     8    35   333    77   492   198     2  2640     1  3393\n",
      "  1720  2634   233    26    84     2   239  2645     1   599     9   621\n",
      "   285   109  1764     4  4215  4272     1    91    72    12     9   207\n",
      "  1643    39    22   319     7   198    21     2  8451    16    31    76\n",
      "   808 18240     7  1086     2   226     1    35  1294     7    11     1\n",
      "   817  1487     7    34    23     1  1281  2249     6     1  1417     2\n",
      "  2645     1   599     3     1  1820    10   285   134    99     5    34\n",
      "   113  1098    90     7    47    91    72   398  1634  3559    12   582\n",
      "    35  1098  1294    26   233   113   896    26    84    47]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = tokenizer.texts_to_sequences(val_reviews)\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(test_reviews)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5027\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['awesome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "The model to classify the text is relatively simple. </br>\n",
    "It first consists of the Embedding Layer to create the Word Vectors.\n",
    "\n",
    "There are two approaches to word vectors - use a already created pre-existing word vector to classify the entities in your dataset or start fresh with a random (empty) embedding matrix and let the model simultaenously come up with its own vectors while fitting the training data.\n",
    "\n",
    "Since the pre-existing word vectors were quite big and cumbersome to deal with, I have only used the fresh / empty embedding approach here. </br>\n",
    "Earlier experiments showed that there wasn't a significant difference in using pre-trained word vectors or this approach </br>\n",
    "\n",
    "The model itself comprises of a AveragePooling layer that takes the average of all the embedding word vectors and converts it into a single vector before sending it further into the network. </br>\n",
    "Further, we only have 2 hidden layers in the Neural Network of 128 and 64 dimensions respectively.\n",
    "This was mainly derived from a hit and trial approach, also taking into account that the embedding matrix had 300 features\n",
    "\n",
    "The output layer only has a single neuron which gives a decimal value (around 0.9+ if it predicts it's Real or around 0.05 if it says it's Fake)\n",
    "\n",
    "Added dropout layers to significantly reduce overfitting on the training data and terminating the model early since this was a observed earlier. </br>\n",
    "Also added Early Stopping (with patience = 3, which could be reduced) to stop the process in case it detects the Valuation and Train Accuracy diverging (Val accuracy decreases but Train accuracy increases), indicating overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rvheb\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rvheb\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\rvheb\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rvheb\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "66/66 [==============================] - 55s 757ms/step - loss: 0.4907 - accuracy: 0.8028 - val_loss: 0.1220 - val_accuracy: 0.9641\n",
      "Epoch 2/20\n",
      "66/66 [==============================] - 44s 660ms/step - loss: 0.0597 - accuracy: 0.9839 - val_loss: 0.0431 - val_accuracy: 0.9890\n",
      "Epoch 3/20\n",
      "66/66 [==============================] - 44s 671ms/step - loss: 0.0192 - accuracy: 0.9963 - val_loss: 0.0351 - val_accuracy: 0.9906\n",
      "Epoch 4/20\n",
      "66/66 [==============================] - 46s 693ms/step - loss: 0.0084 - accuracy: 0.9990 - val_loss: 0.0329 - val_accuracy: 0.9908\n",
      "Epoch 5/20\n",
      "66/66 [==============================] - 43s 651ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.0282 - val_accuracy: 0.9924\n",
      "Epoch 6/20\n",
      "66/66 [==============================] - 44s 669ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0284 - val_accuracy: 0.9926\n",
      "Epoch 7/20\n",
      "66/66 [==============================] - 44s 667ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0292 - val_accuracy: 0.9927\n",
      "Epoch 8/20\n",
      "66/66 [==============================] - 44s 667ms/step - loss: 6.7762e-04 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 0.9926\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(input_dim=num_tokens, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=MAX_LEN))\n",
    "\n",
    "\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
    "model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
    "model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val), callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The model has a very impressive train data accuracy of 99.97% and an even impressive Valuation accuracy of 99.1 % \n",
    "The model can very confidently classify news as fake or real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(reviews):\n",
    "  seqs = tokenizer.texts_to_sequences(reviews)\n",
    "  seqs = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_LEN)\n",
    "  return model.predict(seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = 'Scientists have discovered a massive underground city beneath the Sahara Desert, believed to be inhabited by a lost civilization that possesses advanced technology far beyond modern capabilities. This hidden metropolis, which spans hundreds of miles, is said to be powered by a mysterious energy source that can cure all known diseases and generate limitless clean energy. Archaeologists claim that this civilization has been in contact with extraterrestrial beings, who have guided them in developing technologies that defy the laws of physics. Government officials are allegedly keeping this discovery a secret to prevent global panic and protect the powerful technologies from falling into the wrong hands.'\n",
    "real_news = \"The government should publish advice for its departments on engaging with young people, including on TikTok, a group of MPs has said.\\n\\nThe culture, media and sport committee has been looking into countering disinformation online.\\n\\nIts call comes despite TikTok currently being banned on government devices due to data security concerns.\\n\\nAccurate information needs to be communicated in a \\\"relatable\\\" way, the MPs say.\\n\\nThe committee says that countering misinformation is particularly important for young people, who are increasingly turning away from traditional media and towards social media for their information.\\n\\nIt advises meeting young people \\\"where they are\\\" - with 15 to 24 year olds spending around an hour per day on TikTok, according to media regulator Ofcom.\\n\\nThe report says: \\\"The Government must have a clear strategy for communicating with young people and adapting to the development of new apps and platforms which appeal to this audience.\\\"\\n\\nSome MPs do still use TikTok, despite the fact it is blocked on the Parliament Wi-Fi network.\\n\\nDefence Secretary Grant Shapps has almost 20,000 followers, though he says he does not have the app on his personal phone.\\n\\n\\\"Grant knows that TikTok can be a valuable tool for communicating with his constituents,\\\" a source close to Mr Shapps told the BBC in September 2023.\\n\\nThe Ministry of Defence also operates a separate account which has about 17,500 followers.\\n\\nTikTok is under pressure in many countries over its links to the Chinese state - links it has always denied - with law-makers in the US recently passing legislation saying it should be sold or banned.\\n\\nIt has though endorsed the committee's findings.\\n\\n\\\"We welcome this report's recommendation that the Government should engage with the public on whatever platform they choose to use\\\", it said in a statement.\\n\\nThe government has responded to the committee report by saying it makes efforts to \\\"reach people directly on the platforms they spend the most time on.\\\"\\n\\nIt adds that the Online Safety Act, which came into law last year, \\\"will also help tackle the root cause of disinformation\\\" by requiring social media companies \\\"to swiftly remove illegal misinformation and disinformation as soon as they become aware of it.\\\"\\n\\nBut the law was criticised at the time by fact-checking service Full Fact, which said it did not go far enough \\\"to address the way that platforms treat harmful misinformation and disinformation.\\\"\\n\\nThe MPs took evidence from over 60 different people prior to publishing their report, including disinformation experts and journalists.\\n\\nAmong these were BBC journalists Rebecca Skippage and Marianna Spring.\\n\\nAlso interviewed were financial journalist Martin Lewis, Channel 4 journalist Georgina Lee and the chief executive of Full Fact Will Moy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 345ms/step\n",
      "[[0.0015998]\n",
      " [0.9991107]]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment([fake_news, real_news]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results of the model sample news articles obtained online. </br>\n",
    "\n",
    "The real news was obtained from a BBC news article on Fake News :) </br>\n",
    "The fake news was generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 8ms/step - loss: 0.0189 - accuracy: 0.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.018902253359556198, 0.995100200176239]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2298\n",
      "           1       0.99      0.99      0.99      2192\n",
      "\n",
      "    accuracy                           1.00      4490\n",
      "   macro avg       1.00      1.00      1.00      4490\n",
      "weighted avg       1.00      1.00      1.00      4490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2287   11]\n",
      " [  11 2181]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvheb\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Saving the model\n",
    "model.save('fake_news_classifier.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the tokenizer using joblib\n",
    "joblib.dump(tokenizer, 'tokenizer.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
